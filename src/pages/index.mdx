---
layout: ../layouts/Layout.astro
title: "MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation"
description: "Project page for the paper 'MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation'"
favicon: favicon1.svg
thumbnail: mars_pipe_overview.svg
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import ImageSlider from '../components/ImageSlider.tsx';
import { Icon } from "astro-icon/components";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"
import marsPipeOverview from "../assets/mars_pipe_overview_new.svg";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Stefano Samele",
      url: "https://scholar.google.com/citations?user=9eINCUAAAAAJ&hl=it",
      institution: "Politecnico di Milano",
      notes: ["*"],
    },
    {
      name: "Nico Catalano",
      url: "https://scholar.google.com/citations?user=ZrGxR2YAAAAJ&hl=en",
      institution: "Politecnico di Milano",
      notes: ["*"],
    },
    {
      name: "Paolo Pertino",
      url: "https://scholar.google.com/citations?user=sGhbVfEAAAAJ&hl=en",
      institution: "Politecnico di Milano",
      notes: ["*"],
    },
    {
      name: "Matteo Matteucci",
      url: "https://scholar.google.it/citations?user=PdbEg5YAAAAJ&hl=it",
      institution: "Politecnico di Milano",
    },
  ]}
  conference="? Conference Name ?"
  notes={[
    {
      symbol: "*",
      text: "Equal contribution",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/paolopertino/MARS-Multimodal-Alignment-and-Ranking-System-for-Few-Shot-Segmentation",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2504.07942",
      icon: "academicons:arxiv",
    },
  ]}
  />

<Figure>
  <Image slot="figure" source={marsPipeOverview} altText="High level overview of the MARS pipeline." />
  <span slot="caption">High level overview of the MARS pipeline.</span>
</Figure>

<HighlightedSection>

## Abstract

Current Few Shot Segmentation literature lacks a mask selection method that goes beyond visual similarity between the query and example images, leading to suboptimal predictions. We present MARS, a plug-and-play ranking system that leverages multimodal cues to filter and merge mask proposals robustly. Starting from a set of mask predictions for a single query image, we score, filter, and merge them to improve results. Proposals are evaluated using multimodal scores computed at local and global levels. Extensive experiments on COCO-20i, Pascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring components is crucial for robust ranking, validating our contribution. As MARS can be effortlessly integrated with various mask proposal systems, we deploy it across a wide range of top-performer methods and achieve new state-of-the-art results on multiple existing benchmarks. Code will be available upon acceptance.

</HighlightedSection>

## Pipeline overview
The MARS framework enhances few-shot segmentation by integrating multimodal information into a unified pipeline. It begins with a module that retrieves the essential textual cues from the support images. Using ViP-LLaVA, the system extracts the class name and obtains a detailed description via WordNet. In scenarios with multiple support images, a majority voting mechanism ensures that the consolidated textual information is both robust and reliable.

Following this, the system aligns visual and textual modalities. A pre-trained CLIP model processes the query image along with two specialized text prompts—one emphasizing the presence of the object and one its absence—to generate an initial saliency map that highlights regions of interest. This map is refined by incorporating prior information, leading to a more precise localization of the target region.

To address situations where visual features may not fully capture semantic nuances, the AlphaCLIP module is employed. This module fuses the class name with its detailed textual description to create a robust global conceptual score. By comparing the normalized image and text embeddings, it provides a semantic measure that verifies whether the mask proposals are aligned with the expected class characteristics.

In parallel, the framework processes visual information exclusively. A Vision Transformer extracts detailed features from both the support and query images, producing metrics that capture overall visual similarity as well as fine-grained local correspondences. This results in the computation of both a global visual score and a local visual score, ensuring that the segmentation captures both holistic appearance and detailed structure.

Finally, all the computed scores are integrated in a filtering and merging stage. The Filtering-Merging Module combines the contributions of the local and global conceptual and visual scores into a single MARS score for each mask proposal. Through a dual-threshold strategy, low-confidence proposals are discarded and the remaining ones are merged to produce the final, refined segmentation mask.

## Quantitative results
MARS, being a plug-and-play module, has been tested when applied to a variety of state-of-the-art few-shot segmentation methods, both under the one-shot and five-shot settings. 

The results are shown in the tables below. The evaluation suite is composed of the <LaTeX inline formula="\text{COCO-20}^i" />, <LaTeX inline formula="\text{Pascal-5}^i" />, <LaTeX inline formula="\text{LVIS-92}^i" />, and <LaTeX inline formula="\text{FSS-100}" /> datasets. The results are reported in terms of the mean mean Intersection over Union (mIoU) metric.

<table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%;">
  <thead>
    <tr>
      <th>Method</th>
      <th colspan="2"><LaTeX inline formula="\text{COCO-20}^i" /></th>
      <th colspan="2"><LaTeX inline formula="\text{Pascal-5}^i" /></th>
      <th colspan="2"><LaTeX inline formula="\text{LVIS-92}^i" /></th>
      <th colspan="2"><LaTeX inline formula="\text{FSS-1000}" /></th>
    </tr>
    <tr>
      <th></th>
      <th>Original</th>
      <th>+ MARS</th>
      <th>Original</th>
      <th>+ MARS</th>
      <th>Original</th>
      <th>+ MARS</th>
      <th>Original</th>
      <th>+ MARS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PerSAM</td>
      <td>21.4</td>
      <td><strong>28.8 (+7.4) </strong></td>
      <td>43.1</td>
      <td><strong>56.0 (+12.9) </strong></td>
      <td>12.3</td>
      <td><strong>13.6 (+1.3) </strong></td>
      <td>75.0</td>
      <td><strong>79.1 (+4.1) </strong></td>
    </tr>
    <tr>
      <td>VRP-SAM</td>
      <td>50.1</td>
      <td><strong>52.6 (+2.5) </strong></td>
      <td>69.2</td>
      <td>67.6 (-1.6)</td>
      <td>-</td>
      <td>-</td>
      <td>87.9</td>
      <td>86.4 (-1.5)</td>
    </tr>
    <tr>
      <td>SegGPT</td>
      <td>54.5</td>
      <td><strong>59.9 (+5.4) </strong></td>
      <td>83.2</td>
      <td><strong><u>84.1 (+0.9)</u> </strong></td>
      <td>20.8</td>
      <td><strong>24.0 (+3.2) </strong></td>
      <td>83.3</td>
      <td><strong>84.3</strong> (+1.0)</td>
    </tr>
    <tr>
      <td>Matcher</td>
      <td>52.7</td>
      <td><strong>60.5 (+7.8) </strong></td>
      <td>68.1</td>
      <td><strong>77.2 (+9.1) </strong></td>
      <td>33.0</td>
      <td><strong>36.9 (+3.9) </strong></td>
      <td>87.0</td>
      <td>85.4 (-1.6)</td>
    </tr>
    <tr>
      <td>GF-SAM</td>
      <td>58.7</td>
      <td><u><strong>61.9 (+3.2) </strong></u></td>
      <td>72.1</td>
      <td><strong>75.7 (+3.6) </strong></td>
      <td>35.2</td>
      <td><u><strong>38.7 (+3.5) </strong></u></td>
      <td><u>88.0</u></td>
      <td>87.0 (-1.0)</td>
    </tr>
  </tbody>
  <caption style="caption-side: bottom; margin-top: 10px; text-align: left;">**Table 1: One-Shot Segmentation Results**. For each dataset, the table reports the original performance and the performance after applying MARS in terms of mIoU. In each column, we underlined the top performer method for a particular dataset.</caption>
</table>

<table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%;">
  <thead>
    <tr>
      <th>Method</th>
      <th colspan="2"><LaTeX inline formula="\text{COCO-20}^i" /></th>
      <th colspan="2"><LaTeX inline formula="\text{Pascal-5}^i" /></th>
      <th colspan="2"><LaTeX inline formula="\text{LVIS-92}^i" /></th>
      <th colspan="2"><LaTeX inline formula="\text{FSS-1000}" /></th>
    </tr>
    <tr>
      <th></th>
      <th>Original</th>
      <th>+ MARS</th>
      <th>Original</th>
      <th>+ MARS</th>
      <th>Original</th>
      <th>+ MARS</th>
      <th>Original</th>
      <th>+ MARS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SegGPT</td>
      <td>61.2</td>
      <td><strong>64.3 (+3.1) </strong></td>
      <td>86.8</td>
      <td><u><strong>87.8 (+1.0) </strong></u></td>
      <td>22.4</td>
      <td><strong>23.5 (+0.9) </strong></td>
      <td>86.2</td>
      <td><strong>86.3 (+0.1) </strong></td>
    </tr>
    <tr>
      <td>Matcher</td>
      <td>60.7</td>
      <td><strong>63.6 (+2.9) </strong></td>
      <td>74.0</td>
      <td><strong>80.7 (+6.7) </strong></td>
      <td>40.0</td>
      <td><strong>40.5 (+0.5) </strong></td>
      <td><u>89.6</u></td>
      <td>87.6 (-2.0)</td>
    </tr>
    <tr>
      <td>GF-SAM</td>
      <td>66.8</td>
      <td><u><strong>67.8 (+1.0) </strong></u></td>
      <td>82.6</td>
      <td>81.5 (-1.1)</td>
      <td>44.0</td>
      <td><u><strong>46.7 (+2.7) </strong></u></td>
      <td>88.9</td>
      <td>87.5 (-1.4)</td>
    </tr>
  </tbody>
  <caption style="caption-side: bottom; margin-top: 10px; text-align: left;">**Table 2: Five-Shot Segmentation Results**. or each dataset, the table reports the original performance in terms of mIoU and the performance after applying MARS. In each column, we underlined the top performer method for a particular dataset.</caption>
</table>

## Qualitative Results on the <LaTeX inline formula="\text{COCO-20}^i" /> dataset

<ImageSlider images={[
  "/MARS-Multimodal-Alignment-and-Ranking-System-for-Few-Shot-Segmentation/QualitativeResults.svg",
  "/MARS-Multimodal-Alignment-and-Ranking-System-for-Few-Shot-Segmentation/QualitativeResults_2.svg",
  "/MARS-Multimodal-Alignment-and-Ranking-System-for-Few-Shot-Segmentation/QualitativeResults_3.svg",
  ]}
  interval={8000}
  client:load
/>


## BibTeX citation

```bibtex
@misc{catalano2025marsmultimodalalignmentranking,
      title={MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation}, 
      author={Nico Catalano and Stefano Samele and Paolo Pertino and Matteo Matteucci},
      year={2025},
      eprint={2504.07942},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.07942}, 
}
```